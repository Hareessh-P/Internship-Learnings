
# Day __ (10/6/24)

### Paraquet form of data :
- while reading from 
	- cache lines (64 bytes is the granularity)
	- OS 4kb is the granularity
	- network - ..
- Sequential memory accesses provide a significant performance boost.
- row based --> OLTP(Transaction processing frequent updates, inserts , deletes), nice locality.
- col based --> OLAP(Analytics processing)
- if we want some subsets of rows to be reconstructed while working on a large set of cols, we can use hybrid model
- ![Pasted image 20240610114025](pics/Pasted%20image%2020240610114025.png?raw=true "Pasted image 20240610114025")
 Hybrid storage ^
- Parquet and ORC  uses both horizontal and vertical partitioning to provide hybrid model
### Parquet data organization :
- ![Pasted image 20240610114117](pics/Pasted%20image%2020240610114117.png?raw=true "Pasted image 20240610114117")



### Paraquet encoding schemes :
1.) PLAIN
![Pasted image 20240610114255](pics/Pasted%20image%2020240610114255.png?raw=true "Pasted image 20240610114255")

2.) RLE_DICTIONARY :
- Run length encoding, bit-packing, dictionary compression :
- ![Pasted image 20240610114414](pics/Pasted%20image%2020240610114414.png?raw=true "Pasted image 20240610114414")
- bit-packing --> we can use 2 bits to address the 4 dictionary values..
- Each col has a dict, and if the dict size becomes too large then the Parquet fallback to PLAIN.
- to increase max dict size, 
### Page Compression :
- trade off b/w decompression speed and I/O savings
- page compression using snappy, gzip

### Partitioning and Bucketing :
#### Partitioning :
Dividing large data into smaller manageable parts(partitions) based on 1 or more cols.
Each partition is stored separately, typically in different files or directories.
- embed predicates in directory structure --> if uk that date will an important selector then, we can partition by date.. -->`df.write.partitionBy("date").paraquet(...)`
- "Partitioning is useful when you frequently run queries that filter data based on the partitioned columns."
- "For example, if you have a large dataset of transactions and you often query transactions by date, partitioning by the date column would be beneficial."
	- pros :
		- query optimization, reduced i/o (reads only necessary partitions)
	- cons :
		- can lead to large no of small files if cardinality of partition col is high..
#### Bucketing :
Divides data into fixed no of buckets based on the hash of 1 or more cols.
bucketing --> divs data into *fixed* no of buckets, partitioning --> divs data into directories.
- PROS :
	- Improves performance of `joins and aggregations` by *reducing shuffle data*.
- cons : no of buckets is *fixed*.
#### Comparison :
- **Concept**:
    
    - **Partitioning**: Divides data into directories based on partition column values.
    - **Bucketing**: Divides data into a fixed number of buckets based on the hash of the bucket column(s).
- **Use Case**:
    
    - **Partitioning**: Effective for filtering queries.
    - **Bucketing**: Effective for optimizing joins and aggregations.
- **Data Layout**:
    
    - **Partitioning**: Creates directories for each partition value.
    - **Bucketing**: Creates a fixed number of files (buckets) without additional directories.
- **Flexibility**:
    
    - **Partitioning**: Dynamically adapts to the values in the partition column.
    - **Bucketing**: Requires specifying the number of buckets upfront.
- **Optimization**:
    
    - **Partitioning**: Optimizes I/O by reducing the data read for filtered queries.
    - **Bucketing**: Optimizes data shuffling and sorting, particularly in joins and aggregations.
### Avoid many small files (Small Files Issue) :
- for every file : 
	- set up internal data structures
	- instantiate reader objects
	- fetch file 
	- parse paraquet metadata
- Solution : 
	
```
	- Manual compaction : 
		- df.repartition (numOfPartitions) .write.paraquet (...)
		- df.coalesce (numOfPartitions) .write.paraquet (...)
		- `coalesce` tries to minimize data movement by only moving data when necessary, making it more efficient than `repartition`.
	
	- Partitioning and bucketing :
		 df.write.bucketBy(10, "column_name").saveAsTable("bucketed_table")
			- When writing data to storage, use partitioning or bucketing techniques to organize data into larger units, such as partitions or buckets
			
	- Dynamic File writing :
	
		 df.write.option("maxRecordsPerFile", 1000000).parquet("output_path")
		
			- Use dynamic file writing techniques that adaptively manage file sizes based on the size of incoming data. For example, you can set thresholds for file sizes and automatically create new files when the threshold is reached to avoid excessively small files.
			
	- Optimize Read and Write Operations
			- Optimize read and write operations to minimize overhead when dealing with small files. This may involve using buffered I/O, caching, or optimizing file system parameters to improve performance. 
			
	- Data Lake Architectre :
			- Consider using a data lake architecture that decouples storage from compute and allows for more flexible data organization and management
		
```
`Watchout for incremental workload output...
`

Ex : we have an ETL job (extract, transform, load) --> for every few hrs it appends a file

### Optimizing read write operations :
- Read :
	- Partition pruning --> skips irrelevant partitions while quering
	- Column pruning --> val df = spark.read.parquet("hdfs://path/to/data").select("col1", "col2") , selecting the needed cols alone.
	- Predicate pushdown --> .filter (col1 > 1000)
	- File formats --> ORC or Paraquet
	- Broadcast joins --> - For small tables, use broadcast joins to avoid shuffling large amounts of data across the network.
```
val smallDF = spark.read.parquet("hdfs://path/to/small_data") 
val largeDF = spark.read.parquet("hdfs://path/to/large_data") 
val joinedDF = largeDF.join(broadcast(smallDF), "join_column")
```

	- Caching : 
			 Cache intermediate DataFrames when you need to reuse them in multiple actions. This can avoid reading data multiple times.

```
val df = spark.read.parquet("hdfs://path/to/data")
df.cache()
val result1 = df.filter("col1 > 1000").count()
val result2 = df.filter("col2 < 2000").count()
```

- Write :
	- partitioning n bucketing 
	- compression
	- data coalesce
	- 
### Compression : 
- reduced data vol --> reduced i/o == sequential access
### Data Partition :
- **Partition Pruning**: Partitioning data into smaller, manageable chunks allows query engines to perform partition pruning
- Data Colocation, Parallel Processing, Data Skew Mitigation


## Apache spark : 
### Data Representation: 
- Within each partition, data is typically stored in a serialized format, such as binary or encoded formats like Apache Arrow. 
- This serialized representation allows Spark to efficiently handle data serialization and deserialization across different nodes of the cluster.
### Why do we need Serialization ?
Quoting from `Designing Data Intensive Applications` book:

> Programs usually work with data in (at least) two different representations:
> 
>1. In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so on. These data structures are optimized for efficient access and manipulation by the CPU (typically using pointers).
> 2. When you want to write data to a file or send it over the network, you have to encode it as some kind of self-contained sequence of bytes (for example, a JSON document). Since a pointer wouldn’t make sense to any other process, this sequence-of-bytes representation looks quite different from the data structures that are normally used in memory. Thus, we need some kind of translation between the two representations. The translation from the in-memory representation to a byte sequence is called encoding (also known as serialization or marshalling), and the reverse is called decoding (parsing, deserialization, unmarshalling).

Technically on the low-level, your serialized object will also end up as a stream of bytes on your cable or your filesystem...

So you can also think of it as a standardized and already available way of converting your objects to a stream of bytes. Storing/transferring object is a very common requirement, and it has less or little meaning to reinvent this wheel in every application.

As other have mentioned, you also know that this object->stream_of_bytes implementation is quite robust, tested, and generally architecture-independent.

This does not mean it is the only acceptable way to save or transfer an object: in some cases, you'll have to implement your own methods, for example to avoid saving unnecessary/private members (for example for security or performance reasons). But if you are in a simple case, you can make your life easier by using the serialization/deserialization of your framework, language or VM instead of having to implement it by yourself.

--------------------------------------------------------------------------

### Handling memory in Spark :
- Memory management in spark applications benefitted by the `Lazy Evaluation` Technique..
- Garbage collector : is of JVMs, and if an old dataframe is not referenced then its memory will be reclaimed by JVM
- In-memory storage : u can persist some dfs on the main memory --> 
	- `.cache() or .persist()`
	- shd be used cautiously n freed up when used..
	- `.unpersist()` to free up cached df.



